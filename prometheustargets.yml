# my global config
global:
  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: "prometheus"

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
      - targets: ["10.1.7.132:9101", "10.100.0.10:9100", "10.2.0.40:9100",  "10.3.0.60:9100", "10.4.0.10:9100", "10.5.0.10:9100", "10.6.0.60:9100", "10.7.0.10:9100", "10.9.0.70:9100", "10.10.0.60:9100", "10.11.0.10:9100", "10.12.0.55:9100", "10.13.0.51:9100", "10.14.0.51:9100", "10.15.0.8:9100", "10.16.0.10:9100", "10.20.0.10:9100", "10.21.0.10:9100", "10.22.0.10:9100", "10.23.0.10:9100", "10.23.0.10:9100",  "10.24.0.10:9100", "10.98.0.10:9100", "10.27.0.10:9100", "10.28.0.10:9100", "10.30.0.10:9100", "10.31.0.10:9100", "10.32.0.10:9100", "10.33.0.10:9100", "10.34.0.10:9100", "10.35.0.10:9100"]
